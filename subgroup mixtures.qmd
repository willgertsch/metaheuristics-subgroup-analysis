---
title: "Subgroup mixture model"
format: html
---

```{r}
library(dplyr)
#library(benchtm)
library(ggplot2)
source("subgroup_mixture.R")
library(metaheuristicOpt)
```

## Introduction

The goal of this subgroup analysis is to find a subset of participants who responded to the treatment. The real purpose is to demonstrate the usefulness of metaheuristics.

## Model

Let $Y$ be the outcome of interest and be distributed according to a 2 component mixture distribution. The first component has mean

$$
\mu_0 = Z\beta_0
$$

where $Z$ is a matrix that contains the main analysis variables, usually a treatment and an intercept, but possibly also covariates. The parameter vector $\beta_0$ contains regression coefficients for the subgroup that didn't receive treatment benefit. The second mixture component models those who did receive benefit from treatment. The mean is

$$
\mu_1 = Z(\beta_0 + \beta_1)
$$

Assuming that $Y$ has pdf $f$ with mixture component pdfs $f_0$ and $f_1$, we can write

$$
f(y|Z, \beta, X, \gamma) = w f_0 + (1-w)f_1
$$

where

$$
w = \text{ilogit}(x^T\gamma)
$$

where $x$ is a vector of subgroup membership covariates and $\gamma$ is the parameter vector.

Therefore we have the log-likelihood

$$
\ell(\beta, \gamma|Z,X) = \sum_{i=1}^n \log\left[ w_if_0(y_i|Z,\beta_0) + (1-w_i)f_1(y_i|Z, \beta)\right]
$$

## Data generation
### Continuous
```{r}
set.seed(1234)
X <- generate_X(1000)

dat = generate_data(
  N = 1000,
  beta0 = c(50, 25),
  beta1 = c(0, 20),
  gamma = c(-1.39, 1.79, rep(0, ncol(X)-2)), # sparse
  type = 'continuous',
  trt_prob = 0.5,
  X = X,
  sigma = c(10, 15)
)
```

Make sure to check the size of the subgroup.
```{r}
dat$class %>% table()
```

```{r}
ggplot(as.data.frame(dat), aes(x = as.factor(class), y = Y)) +
  geom_boxplot() +
  facet_wrap(~Z.2)
```

```{r}
# underlying biomarker
ggplot(as.data.frame(dat), aes(x = as.factor(X.X1), y = Y)) +
  geom_boxplot() +
  facet_wrap(~Z.2)
```

### Binary
```{r}
set.seed(1234)
X <- generate_X(1000)

dat = generate_data(
  N = 1000,
  beta0 = c(-3, 0.7),
  beta1 = c(0, 0.3),
  gamma = c(-1.39, 1.79, rep(0, ncol(X)-2)),
  type = 'binary',
  trt_prob = 0.5,
  X = X
)
```


```{r}
count(as.data.frame(dat), Z.2, class, Y)
```

```{r}
# response proportions by treatment arm
as.data.frame(dat) %>%
  group_by(Z.2) %>%
  summarise(n = n(), Y = sum(Y)) %>%
  mutate(p = Y/n)
```

```{r}
# knowledge of subgroup
as.data.frame(dat) %>%
  group_by(Z.2, class) %>%
  summarise(n = n(), Y = sum(Y)) %>%
  mutate(p = Y/n)
```

```{r}
# treatment effect
glm(Y ~ Z.2, family = binomial, data = as.data.frame(dat)) %>%
  summary()
```

```{r}
# subgroup interaction model
glm(Y ~ Z.2*class, family = binomial, data = as.data.frame(dat)) %>%
  summary()
```

## Count
```{r}
# data generation
set.seed(1234)
X <- generate_X(1000)

dat = generate_data(
  N = 1000,
  beta0 = c(1, 2),
  beta1 = c(0, 1),
  gamma = c(-1.39, 1.79, rep(0, ncol(X)-2)),
  type = 'count',
  trt_prob = 0.5,
  X = X
)
```

```{r}
# can clearly see multimodal
dat$Y %>% hist()
```


## Fitting the model
### Continuous
```{r}
# generate data
set.seed(1234)
X <- generate_X(1000)

dat = generate_data(
  N = 1000,
  beta0 = c(50, 25),
  beta1 = c(0, 20),
  gamma = c(-1.39, 1.79, rep(0, ncol(X)-2)), # sparse
  type = 'continuous',
  trt_prob = 0.5,
  X = X,
  sigma = c(10, 15)
)
```

```{r}
table(dat$class)
```

```{r}
rangeVar = matrix(c(0, 0, -10, 0, -5, -5, -5, 1, 1,
                    100, 50, 10, 50, 5, 5, 5, 50, 50), nrow=2, byrow = T)
# 
# ll_fun = ll_factory('continuous', dat$Y, dat$X, dat$Z)
# result = metaOpt(
#   ll_fun,
#   optimType = 'MAX',
#   algorithm = 'HS',
#   numVar = 9,
#   rangeVar = rangeVar,
#   control = list(
#     numPopulation = 40,
#     maxIter = 500
#   )
# )

set.seed(1124)
mod = fit_model('continuous', dat$Y, dat$X, dat$Z, swarm = 100, maxIter = 500,
                algorithm = 'DE', rangeVar = rangeVar)
```

```{r}
summary.subgroup(mod)
```


### Binary
```{r}
set.seed(1234)
X <- generate_X(1000)

dat = generate_data(
  N = 1000,
  beta0 = c(-3, 0),
  beta1 = c(0, 2),
  gamma = c(-1.39, 1.79, rep(0, ncol(X)-2)),
  type = 'binary',
  trt_prob = 0.5,
  X = X
)
```

```{r}
rangeVar = matrix(c(-5, -5, -5, 0, -5, -5, -5,
                    5, 5, 5, 5, 5, 5, 5), nrow=2, byrow = T)


set.seed(1124)
mod = fit_model('binary', dat$Y, dat$X, dat$Z, swarm = 100, maxIter = 500,
                algorithm = 'DE')
```

```{r}
summary.subgroup(mod)
```
I think the problem has to do with identifibility. See https://core.ac.uk/reader/77047153?utm_source=linkout

The problem is that our distribution produces data that looks like
```{r}
as.data.frame(dat) %>%
  count(Y) %>%
  mutate(p = n/sum(n))
```
This might be from a bernoulli distribution with p = 0.088. The problem is that we only have two possible outcome values.

### Count

```{r}
# data generation
set.seed(1234)
X <- generate_X(1000)

dat = generate_data(
  N = 1000,
  beta0 = c(1, 2),
  beta1 = c(0, 1),
  gamma = c(-1.39, 1.79, rep(0, ncol(X)-2)),
  type = 'count',
  trt_prob = 0.5,
  X = X
)
```

```{r}
rangeVar = matrix(c(-5, -5, -5, 0, -5, -5, -5,
                    5, 5, 5, 5, 5, 5, 5), nrow=2, byrow = T)


set.seed(1124)
mod = fit_model('count', dat$Y, dat$X, dat$Z, swarm = 100, maxIter = 500,
                algorithm = 'DE')
```

```{r}
summary.subgroup(mod)
```
This one seems to work, but may be more difficult than the continuous case still.

## Controlling for baseline
In practice, we usually care more about the covariate adjusted treatment effect. We usually want to adjust for baseline.
```{r}
# generate data
set.seed(1234)
X <- generate_X(1000)

dat = generate_data(
  N = 1000,
  beta0 = c(50, 25, 10),
  beta1 = c(0, 20, 0),
  gamma = c(-1.39, 1.79, rep(0, ncol(X)-2), 0.5), # sparse
  type = 'continuous',
  trt_prob = 0.5,
  X = X,
  sigma = c(10, 15)
)
```

```{r}
lm(Y ~ Z.V2 + Z.baseline_var, data = as.data.frame(dat)) %>%
  summary()
```

Now fit the subgroup model to this data.
```{r}
set.seed(1124)
mod = fit_model('continuous', dat$Y, dat$X, dat$Z, swarm = 100, maxIter = 500,
                algorithm = 'DE')
```

```{r}
summary.subgroup(mod)
```

Seems to do ok, but might need more iterations and/or tuning.
